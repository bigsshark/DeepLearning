{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.VERSION)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(10,activation='relu'))\n",
    "model.add(layers.Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1240fba20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config layer\n",
    "layers.Dense(10,activation=tf.sigmoid)# ''sigmoid\n",
    "# regularization\n",
    "layers.Dense(10,activation='relu',kernel_regularizer=keras.regularizers.l1(0.01))#l2(0.01)\n",
    "# 权重初始化\n",
    "layers.Dense(10,kernel_initializer='orthogonal')\n",
    "# bias 初始化\n",
    "layers.Dense(10,bias_initializer=keras.initializers.constant(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64,input_shape=(32,),activation='relu'),\n",
    "    layers.Dense(64,activation='relu'),\n",
    "    layers.Dense(10,activation='sigmoid')\n",
    "])\n",
    "model.compile('adam',loss='categorical_crossentropy',metric=['accuracy'])\n",
    "# tf.train.RMSPropAdam, Adam\n",
    "# tf.keras.metrics   mean absolute error\n",
    "# tf.keras.losses     mse, binary_crossentropy,\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "model.compile(optimizer, \n",
    "              loss='mse', # mean_square_error\n",
    "              metrics=['mae'])# mean absolute error\n",
    "\n",
    "model.compile(tf.train.RMSPropOptimizer(0.01),\n",
    "                     loss=keras.losses.categorical_crossentropy,\n",
    "                     metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_labels_random(shape):\n",
    "    n,n_class = shape\n",
    "    classes = np.random.randint(0,n_class,n)\n",
    "    labels = np.zeros((n, n_class))  \n",
    "    labels[np.arange(n), classes] = 1\n",
    "    return labels\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = one_hot_labels_random((1000, 10))\n",
    "\n",
    "# model.fit(data, labels, epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = np.random.random((100,32))\n",
    "valid_label = one_hot_labels_random((100,10))\n",
    "\n",
    "# model.fit(data,labels,epochs=10,batch_size=64,validation_data=(valid_data,valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/sample - loss: 2.5205 - acc: 0.0800\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.8420 - acc: 0.3320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.inputdata\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data,labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((valid_data,valid_label))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "# model.fit(dataset,epochs=10,steps_per_epoch=30)\n",
    "# model.fit(dataset,epochs=10,steps_per_epoch=30,validation_data=val_dataset,validation_steps=3)\n",
    "\n",
    "# evalute & predict\n",
    "eval_data = np.random.random((100,32))\n",
    "eval_label = one_hot_labels_random((100,10))\n",
    "model.evaluate(x=eval_data,y=eval_label,batch_size=32)\n",
    "model.evaluate(dataset,steps=32)\n",
    "\n",
    "result = model.predict(eval_data)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 21:39:39.894055 4376765888 training_utils.py:1300] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 2.3170 - acc: 0.0990 - val_loss: 2.3127 - val_acc: 0.0938\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2949 - acc: 0.1239 - val_loss: 2.3111 - val_acc: 0.0521\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2833 - acc: 0.1335 - val_loss: 2.3136 - val_acc: 0.0833\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.2715 - acc: 0.1474 - val_loss: 2.3164 - val_acc: 0.1042\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2639 - acc: 0.1571 - val_loss: 2.3175 - val_acc: 0.1042\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2563 - acc: 0.1741 - val_loss: 2.3180 - val_acc: 0.0938\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2456 - acc: 0.1741 - val_loss: 2.3197 - val_acc: 0.1146\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2356 - acc: 0.1902 - val_loss: 2.3212 - val_acc: 0.0938\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2275 - acc: 0.1966 - val_loss: 2.3241 - val_acc: 0.0938\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2149 - acc: 0.2083 - val_loss: 2.3217 - val_acc: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12d652e80>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build advance model\n",
    "input_x = keras.Input(shape=(32,))\n",
    "\n",
    "x = keras.layers.Dense(64, activation='relu')(input_x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "prdict = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=input_x,outputs=prdict)\n",
    "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(dataset,epochs=10,steps_per_epoch=30,validation_data=val_dataset,validation_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKeras(keras.Model):\n",
    "    \n",
    "    def __init__(self,num_classes):\n",
    "        super(MyKeras,self).__init__(name='mymodel')\n",
    "        self.layer1 = keras.layers.Dense(32,activation='relu')\n",
    "        self.layer2 = keras.layers.Dense(num_classes,activation='sigmoid')\n",
    "    def call(self,inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        return  self.layer2(x)\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        return tf.TensorShape(shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 221us/sample - loss: 2.3362 - acc: 0.1000\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 2.3134 - acc: 0.1020\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 2.3057 - acc: 0.0960\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 2.3010 - acc: 0.1000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 43us/sample - loss: 2.2987 - acc: 0.1110\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 37us/sample - loss: 2.2950 - acc: 0.1080\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 2.2932 - acc: 0.1210\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 2.2908 - acc: 0.1350\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 38us/sample - loss: 2.2887 - acc: 0.1310\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 43us/sample - loss: 2.2870 - acc: 0.1300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12d7a1b70>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyKeras(10)\n",
    "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(data,labels,batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define layers\n",
    "\n",
    "- build: Create the weights of the layer. Add weights with the add_weight method.\n",
    "- call: Define the forward pass.\n",
    "- compute_output_shape: Specify how to compute the output shape of the layer given the input shape.\n",
    "- Optionally, a layer can be serialized by implementing the get_config method and the from_config class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,output_dim,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super(MyLayer,self).__init__(**kwargs)\n",
    "            \n",
    "#             super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.kernel)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                  shape=shape,\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        # Make sure to call the `build` method at the end\n",
    "        super(MyLayer, self).build(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.output_dim\n",
    "        return tf.TensorShape(shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(MyLayer,self).get_confg()\n",
    "        base_config['re'] = self.output_dim\n",
    "        return base_config\n",
    "    @classmethod\n",
    "    def from_config(cls, confg):\n",
    "        return cls(**confg)\n",
    "    \n",
    "# class MyLayer(layers.Layer):\n",
    "\n",
    "#     def __init__(self, output_dim, **kwargs):\n",
    "#         self.output_dim = output_dim\n",
    "#         super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "#         # Create a trainable weight variable for this layer.\n",
    "#         self.kernel = self.add_weight(name='kernel',\n",
    "#                                       shape=shape,\n",
    "#                                       initializer='uniform',\n",
    "#                                       trainable=True)\n",
    "#         # Make sure to call the `build` method at the end\n",
    "#         super(MyLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         shape = tf.TensorShape(input_shape).as_list()\n",
    "#         shape[-1] = self.output_dim\n",
    "#         return tf.TensorShape(shape)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         base_config = super(MyLayer, self).get_config()\n",
    "#         base_config['re'] = self.output_dim\n",
    "#         return base_config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 21:02:42.258763 4376765888 deprecation.py:506] From /Users/xywy/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 188us/sample - loss: 2.3067 - acc: 0.0850\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.2991 - acc: 0.0970\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 33us/sample - loss: 2.2976 - acc: 0.1090\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 2.2959 - acc: 0.1060\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 32us/sample - loss: 2.2940 - acc: 0.1120\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 2.2911 - acc: 0.1120\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.2888 - acc: 0.1210\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 2.2864 - acc: 0.1260\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 2.2845 - acc: 0.1320\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 2.2823 - acc: 0.1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12d700400>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = keras.Sequential([MyLayer(10), keras.layers.Activation('softmax')])\n",
    "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(data,labels,batch_size=32,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 2.2811 - acc: 0.1360 - val_loss: 2.3202 - val_acc: 0.0800\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 35us/sample - loss: 2.2793 - acc: 0.1400 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 35us/sample - loss: 2.2774 - acc: 0.1400 - val_loss: 2.3161 - val_acc: 0.1000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 37us/sample - loss: 2.2745 - acc: 0.1400 - val_loss: 2.3202 - val_acc: 0.0700\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 2.2742 - acc: 0.1540 - val_loss: 2.3171 - val_acc: 0.0600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12dc03320>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.keras.callbacks.ModelCheckpoint: Save checkpoints of your model at regular intervals.\n",
    "# tf.keras.callbacks.LearningRateScheduler: Dynamically change the learning rate.\n",
    "# tf.keras.callbacks.EarlyStopping: Interrupt training when validation performance has stopped improving.\n",
    "# tf.keras.callbacks.TensorBoard: Monitor the model's behavior using TensorBoard.\n",
    "\n",
    "callbacks = [\n",
    "  # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data,labels,validation_data=(valid_data,valid_label),batch_size=32,epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 2.3376 - acc: 0.0920\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 43us/sample - loss: 2.3100 - acc: 0.1150\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 2.3033 - acc: 0.1180\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 2.2944 - acc: 0.1150\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 2.2867 - acc: 0.1240\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 2.2788 - acc: 0.1250\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 2.2713 - acc: 0.1450\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 41us/sample - loss: 2.2666 - acc: 0.1400\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 2.2595 - acc: 0.1400\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 2.2557 - acc: 0.1430\n"
     ]
    }
   ],
   "source": [
    "# save and restore \n",
    "# weight only=\n",
    "model = keras.Sequential([layers.Dense(64, activation='relu', input_shape=(32,)), layers.Dense(10,activation='softmax')])\n",
    "model.compile(tf.train.AdamOptimizer(0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.save_weights('./weights/my_model')\n",
    "model.load_weights('./weights/my_model')\n",
    "\n",
    "model.save_weights('my_model.h5',save_format='h5')\n",
    "model.load_weights('my_model.h5')\n",
    "json_string = model.to_json()\n",
    "import pprint\n",
    "import json\n",
    "# pprint.pprint(json.loads(json_string))\n",
    "# fresh_model = tf.keras.models.model_from_json(json_string)\n",
    "# yaml_string = fresh_model.to_yaml()\n",
    "# fresh_model = tf.keras.models.model_from_yaml(yaml_string)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = keras.Sequential([layers.Dense(64,activation='relu',input_shape=(32,)), layers.Dense(10,activation='softmax')])\n",
    "model.compile('rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(data,labels,batch_size=32,epochs=10)\n",
    "model.save('my_model.h5')\n",
    "model = keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 21:40:21.051130 4376765888 estimator.py:1811] Using temporary folder as model directory: /var/folders/wp/_3j2hkz96tg5_qbz2f_47yzc0000gn/T/tmpmzy4ye3o\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([layers.Dense(64,input_shape=(32,), activation='relu'), layers.Dense(10,activation='softmax')])\n",
    "model.compile('rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 21:53:01.454910 4376765888 cross_device_ops.py:1182] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor dense_132/bias is not found in /tmp/model_dir/keras/keras_model.ckpt checkpoint {'global_step': [], 'dense_121/bias': [1], 'dense_121/kernel': [16, 1], 'dense_120/kernel': [10, 16], 'dense_120/bias': [16]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-df284eff6cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m   model_dir='/tmp/model_dir')\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mkeras_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_distribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_distributed\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1217\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_distribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m       return self._actual_train_model_distributed(\n\u001b[0;32m-> 1219\u001b[0;31m           self._config._train_distribute, input_fn, hooks, saving_listeners)\n\u001b[0m\u001b[1;32m   1220\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_train_model_distributed\u001b[0;34m(self, strategy, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1327\u001b[0m         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1328\u001b[0m                                                \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                                saving_listeners)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m   def _train_with_estimator_spec_distributed(self, estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1363\u001b[0m       logging.info('Warm-starting with WarmStartSettings: %s' %\n\u001b[1;32m   1364\u001b[0m                    (self._warm_start_settings,))\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0mwarm_starting_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warm_start_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow/python/training/warm_starting_util.py\u001b[0m in \u001b[0;36mwarm_start\u001b[0;34m(ckpt_to_initialize_from, vars_to_warm_start, var_name_to_vocab_info, var_name_to_prev_var_name)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mvocabless_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_tensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m   \u001b[0mcheckpoint_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_to_initialize_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabless_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m   prev_var_name_not_used = set(\n\u001b[1;32m    478\u001b[0m       var_name_to_prev_var_name.keys()) - prev_var_name_used\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36minit_from_checkpoint\u001b[0;34m(ckpt_dir_or_file, assignment_map)\u001b[0m\n\u001b[1;32m    286\u001b[0m       ckpt_dir_or_file, assignment_map)\n\u001b[1;32m    287\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cross_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0minit_from_checkpoint_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     distribution_strategy_context.get_replica_context().merge_call(\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    284\u001b[0m   \"\"\"\n\u001b[1;32m    285\u001b[0m   init_from_checkpoint_fn = lambda _: _init_from_checkpoint(\n\u001b[0;32m--> 286\u001b[0;31m       ckpt_dir_or_file, assignment_map)\n\u001b[0m\u001b[1;32m    287\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cross_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0minit_from_checkpoint_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/object_detection/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36m_init_from_checkpoint\u001b[0;34m(ckpt_dir_or_file, assignment_map)\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtensor_name_in_ckpt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariable_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         raise ValueError(\"Tensor %s is not found in %s checkpoint %s\" % (\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mtensor_name_in_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         ))\n\u001b[1;32m    321\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_is_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor dense_132/bias is not found in /tmp/model_dir/keras/keras_model.ckpt checkpoint {'global_step': [], 'dense_121/bias': [1], 'dense_121/kernel': [16, 1], 'dense_120/kernel': [10, 16], 'dense_120/bias': [16]}"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    data = np.random.random((1024,10))\n",
    "    labels = np.random.randint(2,size=(1024,1))\n",
    "    x = tf.cast(data, tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,labels))\n",
    "    dataset = dataset.repeat(10).batch(32)\n",
    "    return dataset\n",
    "\n",
    "strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "\n",
    "keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "  keras_model=model,\n",
    "  config=config,\n",
    "  model_dir='/tmp/model_dir')\n",
    "\n",
    "keras_estimator.train(input_fn=input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
