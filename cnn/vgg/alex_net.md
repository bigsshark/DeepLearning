# AlexNet


## ImageNet Classification with Deep Convolutional Neural Networks

- Alex Krizhevsky University of Toronto  kriz@cs.utoronto.ca
- Ilya Sutskever University of Toronto     ilya@cs.utoronto.ca
- Geoffrey E. Hinton University of Toronto  hinton@cs.utoronto.ca

---
### abstract 摘要

我们训练了一个大的、深的神经网络去分类120万个高精图像，在 ImageNet LSVRC-2010比赛中分为1000个不同的类。
在测试集上，我们实现了前三的错误率和前五错误率分别为37.5%、17%，这比最新的技术水平要好很多。这个神经网络有6千万个参数和65万个神经元，由五个卷积神经网络，有些跟随着max-pooling(池化层)层，然后是3个全连神经网络，最后是1000个softmax。为了加快训练，我们使用了非饱和神经元和一种GPU实现非常搞笑的卷积运算。为了减少全连接层的过拟合，我们采用了最近开发的"dropout"正则化方法，该方法被证明针对过拟合非常有效。我们还在 ILSVRC-2012比赛中用了该模型的变种，并获得了15.3%的top5错误率和26.2%的top错误率。

总结: 目标、达到的效果、网络架构、一些优化技术、利用GPU加快训练

---
### Introduction

当前图像识别的必要方法是机器学习。为了提高模型的表现，我们可以收集更大的数据集，训练更强的模型，使用最好的技术去阻止过拟合。直到现在，标记图像数据集的大小相对较小-大约一万个图像数据(e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12])。特别地使用此大小的数据集h可以很简单的解决图像识别任务，特别是通过保留标签进行图像增强、扩充。例如z当前在手写数字图像数据集上的最后的error达到了0.3%，达到了人类的水平。但是在实际设置中的对象表现出相当大的可变性，因此要学会识别它们，有必要使用更大的训练集。同时，小数据集的特点已经被广泛认识，但是最近才变得可能去收集百万数量级别的labeled datasets。新的更大的数据集包括LabelMe [23]和ImageNet [6]，后者由数十万个完全分割的图像组成，ImageNet [6]包含超过1500万个标记的高分辨率图像，超过22,000个类别。 

为了从上百万的数据集中学习成千个类别，我们需要一个学习容量更大的模型。然而，对象识别任务的巨大复杂性意味着即使像ImageNet这样大的数据集也无法指定这个问题，所以我们的模型也应该有很多先验知识来补偿我们没有的所有数据。卷积神经网络（CNN）构成了一类这样的模型[16,11,13,18,15,22,26]。 它们的容量可以通过改变它们的深度和广度来控制，并且它们也对图像的性质（即统计的平稳性和像素依赖性的局部性）做出强有力且大部分正确的假设。 因此，与具有类似大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而它们的理论上最佳性能可能仅略微更差。

尽管CNN具有吸引人的特性，并且尽管它们的本地架构相对有效，但是它们在大规模应用于高分辨率图像方面仍然过于昂贵。 幸运的是，当前的GPU与高度优化的2D卷积实现相结合，足以促进有趣的大型CNN的训练，而最新的数据集如ImageNet包含足够的标记示例来训练此类模型而不会出现严重的过度拟合。

本文的具体贡献如下：我们迄今为止在ILSVRC-2010和ILSVRC-2012竞赛中使用的ImageNet子集中训练了最大的卷积神经网络之一[2]并取得了迄今为止报道的最佳结果。这些数据集。我们编写了一个高度优化的2D卷积GPU实现以及训练卷积神经网络中固有的所有其他操作，我们公开提供这些操作1。我们的网络包含许多新的和不寻常的功能，可以改善其性能并减少其培训时间，详见第3节。我们的网络规模过大，即使有120万个标记的培训示例，我们也使用了几个防止过度拟合的有效技术，见第4节。我们的最终网络包含五个卷积层和三个完全连接层，这个深度似乎很重要：我们发现去除任何卷积层（每个包含不超过模型参数的1％）导致性能较差。

最后，网络的大小主要受当前GPU可用内存量以及我们愿意容忍的培训时间限制。 我们的网络需要五到六天才能在两台GTX 580 3GB GPU上进行训练。 我们所有的实验都表明，只需等待更快的GPU和更大的数据集可用，我们的结果就可以得到改善。

总结: 小数据集合的问题广泛认知、需要收集大量的高精度数据、为了训练大量数据需要一个更强的模型CNN、CNN更容易训练有更少的参数、理论上效率略差。CNN训练困难，幸运的是GPU和高度优化的2D卷积实现。本文的贡献:训练了一个到目前为止最好的效果，高度优化的2d卷积gpu训练，新功能减少训练时间，过拟合技术。
<font color='r'>
-  http://code.google.com/p/cuda-convnet/ gpu代码实现

---
### dataset

ImageNet是超过1500万个标记的高分辨率图像的数据集，大约有22,000个类别。 这些图像是从网上收集的，并由人类贴标机使用Ama- zon的Mechanical Turk众包工具进行标记。 从2010年开始，作为Pascal视觉对象挑战赛的一部分，举办了名为ImageNet大规模视觉识别挑战赛（ILSVRC）的年度比赛。 ILSVRC使用ImageNet的一个子集，在1000个类别中分别拥有大约1000个图像。 总之，大约有120万个训练图像，50,000个验证图像和150,000个测试图像。

ILSVRC-2010是唯一可以获得测试集标签的ILSVRC版本，因此这是我们执行大部分实验的版本。 由于我们也在ILSVRC-2012竞赛中输入了我们的模型，因此我们在第6节中报告了此版本数据集的结果，其中测试集标签不可用。 在ImageNet上，习惯上报告两个错误率：top-1和top-5，其中前5个错误率是测试图像的分数，正确的标签不属于模型认为最可能的五个标签之中。

ImageNet由可变分辨率图像组成，而我们的系统需要恒定的输入维度。 因此，我们将图像下采样到256⇥256的固定分辨率。给定矩形图像，我们首先重新缩放图像，使得较短边长度为256，然后从结果中裁剪出中心256⇥256补丁。 图片。 我们没有以任何其他方式预处理图像，除了从每个像素减去训练集上的平均值。 因此，我们在像素的（居中）原始RGB值上训练我们的网络。

- 训练集规模
- 训练集的评估指标top1,top5
- 裁剪到固定维度，减去训练集上的均值，除此以外没有其他任何预处理操作

---
### The Architecture
我们网络的体系结构如图2所示。它包含8个学习层 - 五个卷积和三个完全连接。 下面，我们将介绍我们网络架构的一些新颖或不寻常的功能。 第3.1-3.4节根据我们对其重要性的估计进行排序，其中最重要的是第一部分。

##### Relu nonlinearity
将神经元输出f建模为其输入x的函数的标准方法是$f（x）= tanh（x）$或$f（x）=（1 + ex）1$。 就具有梯度下降的训练时间而言，这些饱和非线性比非饱和非线性f（x）= max（0，x）慢得多。 继Nair和Hinton [20]之后，我们将具有这种非线性的神经元称为整流线性单位（ReLUs）。 与ReLU相比，深度卷积神经网络的训练速度比tanh单位的速度快几倍。 图1展示了这一点，图1显示了特定四层卷积网络在CIFAR-10数据集上达到25％训练误差所需的迭代次数。 该图表明，如果我们使用传统的饱和神经元模型，我们就无法用这种大型神经网络进行实验。

我们不是第一个考虑CNN传统神经元模型的替代品。 例如，Jarrett等人。 [11]声称非线性f（x）= | tanh（x）| 特别适用于对比度标准化，然后在Caltech-101数据集上进行局部平均汇总。 但是，在这个数据集上，主要关注的是防止过度拟合，因此他们观察到的效果不同于我们在使用ReLU时报告的训练集的加速能力。 更快的学习对在大型数据集上训练的大型模型的性能有很大影响。

具有ReLU（实线）的四层卷积神经网络在CIFAR-10上达到25％的训练错误率，比具有tanh神经元的等效网络（虚线）快六倍。 每个网络的学习率都是独立选择的，以便尽快进行培训。 没有采用任何形式的正规化。 这里展示的效果的大小因网络架构而异，但是具有ReLU的网络通常比具有饱和神经元的等效物快几倍。

#### Training on Multiple GPUs

单个GTX 580 GPU只有3GB内存，这就限制了能在改gpu上训练的网络最大值。毫无疑问120万个训练样本足以训练网络但是太大而无法在单台gpu上去训练。因此我们就传播网络在两台GPU上，当前的GPU特别适合跨GPU并行化，因为它们能够直接读取和写入彼此的内存，而无需通过主机内存。我们采用的并行化方案基本上将一半内核（或神经元）放在每个GPU上，还有一个额外的技巧：GPU仅在某些层中进行通信。这意味着，例如，第3层的内核从第2层中的所有内核映射获取输入。但是，第4层中的内核仅从位于同一GPU上的第3层中的那些内核映射获取输入。 选择连通模式是交叉验证的一个问题，但这使我们能够精确调整通信量，直到它是计算量的可接受部分。

得到的结构有点类似于Cireşan等人使用的“柱状”CNN的结构。 [5]，除了我们的列不是独立的（见图2）。 与在一个GPU上训练的每个卷积层中具有一半内核的网相比，该方案将我们的前1和前5错误率分别降低了1.7％和1.2％。 与双GPU net2相比，双GPU网络训练时间略短。

- 跨gpu训练方案
- 得到的结果

#### Local Response Normalization

ReLU具有理想的属性，它们不需要输入规范化以防止它们饱和。 如果至少一些训练样例对ReLU产生积极的输入，那么将在该神经元中进行学习。 但是，我们仍然发现以下局部归一化方案有助于推广。 由aix表示，y通过在位置处应用内核i计算的神经元的活动
（x，y）然后应用ReLU非线性，响应标准化活动bix，y由表达式给出:

当和在n个“相邻”的核映射上在相同的空间位置上运行时，n是层中的核总数。当然，内核映射的顺序是任意的，并在训练开始前确定。这种反应标准化实现了一种由实际神经元中发现的类型所激发的横向抑制形式，在使用不同核计算的神经元输出之间产生对大活动的竞争。常数k、n和是超参数，其值是使用验证集确定的；我们使用k=2、n=5、=104和=0.75。我们在某些层中应用了relu非线性后应用了这种归一化（见第3.5节）。

该方案与Jarrett等人的局部对比标准化方案有些相似。[11]，但是我们的“亮度归一化”会更准确地称为“亮度归一化”，因为我们不减去平均活动。响应标准化将我们的前1和前5错误率分别降低1.4%和1.2%。我们还验证了该方案在CIFAR-10数据集上的有效性：一个四层CNN在没有标准化的情况下获得13%的测试错误率，在标准化的情况下获得11%。



#### Overlapping Pooling

CNN中的聚集层总结了同一核映射中相邻神经元群的输出。传统上，由相邻汇集单元汇总的社区不会重叠（例如[17，11，4]）。更精确地说，池层可以被认为是由一个间隔s像素的池单元网格组成，每个单元汇总一个以池单元位置为中心的z z大小的邻域。如果我们设置s=z，我们将获得CNN中常用的传统本地池。如果我们设置s<z，我们得到重叠池。这是我们在整个网络中使用的，S=2和Z=3。与非重叠方案s=2，z=2相比，该方案将前1和前5错误率分别降低了0.4%和0.3%，从而产生等效尺寸的输出。我们通常在培训过程中观察到，具有重叠池的模型发现稍微难以过拟合。


#### Overall Architecture

现在我们准备去描述我们的CNN架构，像图2描述的那样，网络包含8个layer和权重。前五个是卷积层，紧跟着3个全连接层。最后一层的输出是通过1000个softmax，产生1000个类的标签的分布。我们的网络最大化多项逻辑回归目标，这相当于最大化预测分布下正确标签的对数概率的训练案例的平均值。

第二，第四和第五卷积层的内核仅连接到位于同一GPU上的前一层中的那些内核映射（参见图2）。 第三卷积层的内核连接到第二层中的所有内核映射。 完全连接的层中的神经元连接到前一层中的所有神经元。 响应标准化层遵循第一和第二卷积层。 3.4节中描述的最大池化层遵循响应归一化层和第五卷积层。 ReLU非线性应用于每个卷积和完全连接层的输出。

第一个卷积层使用96个大小为11⇥11⇥3的内核过滤224×224×3输入图像，步长为4个像素（这是内核映射中相邻神经元的感知场中心之间的距离）。 第二个卷积层将第一个卷积层的（响应标准化和合并）输出作为输入，并用256个大小为5×5×48的内核对其进行过滤。第三个，第四个和第五个卷积层相互连接而没有 任何介入池或标准化层。 第三卷积层具有384个大小为3×3×256的内核，其连接到第二卷积层的（归一化的，合并的）输出。 第四个卷积层有384个大小为3×3×192的内核，第五个卷积层有256个大小为3×3×192的内核。完全连接的层各有4096个神经元。

---
### Reducing Overfitting

 
#### Data Augmentation
减少图像过度拟合最简单最常用的方式是人为的去增大数据集使用标签保留转换技术。我们采用两种不同形式的数据增强，这两种形式都允以非常小的计算从原始图像生成变换图像，因此转换后的图像不需要存储在磁盘上。在我们的实现中，转换的图像是在CPU上的Python代码中生成的，而GPU正在训练前一批图像。 因此，这些数据增强方案实际上在计算上是免费的。

第一种形式的数据增强包括生成图像平移和水平反射。我们通过从256⇥256图像中提取随机224⇥224个补丁（及其水平反射）并在这些提取的补丁4上训练我们的网络来实现这一点。这使我们的训练集的大小增加了2048倍，尽管由此产生的训练样例当然是高度相互依赖的。如果没有这种方案，我们的网络就会遭受严重的过度拟合，这将迫使我们使用更小的网络。 在测试时，网络通过提取五个224×224个补丁（四个角补丁和中心补丁）以及它们的水平反射（因此总共十个补丁）进行预测，并对网络的softmax层进行的预测求平均值 十个补丁。

第二种形式的数据增强包括改变训练图像中RGB通道的强度。具体来说，我们在整个ImageNet训练集中对RGB像素值集执行PCA。对于每个训练图像，我们添加多个找到的主成分，其幅度与相应的特征值成比例，乘以从中得出的随机变量高斯，平均零和标准差0.1。针对每一个RGB图像像素$$I_{xy}=\left[ I_{xy}^{R},\; I_{xy}^{G},I_{xy}^{B} \right]^{T}$$, 我们加上下面的量: $$\left[ p_{1},p_{2},p_{3} \right]\left[ \alpha _{1}\lambda _{1},\alpha _{2}\lambda _{2},\alpha _{3}\lambda _{3} \right]^{T}$$
其中pi和拉姆达i分别是RGB像素值的3*3的协方差矩阵中的特征向量和特征值。ai是我们前面提到的随机变量。对于特定训练图像的所有像素，每个i只绘制一次，直到该图像再次用于训练，然后重新绘制。该方案近似地捕获了自然图像的一个重要特性，即对象标识不受光照强度和颜色变化的影响。该方案将前1位的错误率降低了1%以上。


#### Dropout
结合多个不同模型的预测是一个成功的方法去减少测试错误，但是针对需要训练几天的大的神经网络这个花费太大了。然而，有一个非常有效的模型组合版本，在培训期间只需要花费大约两倍的成本。最近引入的一种技术称为“dropout”[10]，它将每个隐藏神经元的输出设置为0的概率设置为0.5。以这种方式“脱落”的神经元不参与正向传递，也不参与反向传播。因此，每次输入被提出时，神经网络都会对不同的体系结构进行采样，
但所有这些体系结构都共享权重。这种技术减少了神经元复杂的协同适应，因为神经元不能依赖于特定的其他神经元的存在。因此，它被迫学习与其他神经元的许多不同随机子集一起使用的更健壮的特性。在测试时，我们使用所有的神经元，但将它们的输出乘以0.5，
这是一个合理的近似，取由指数型多退出网络产生的预测分布的几何平均值。我们在图2的前两个完全连接的层中使用dropout。没有退出，我们的网络前hibits大量过度拟合。Dropout大约是收敛所需迭代次数的两倍。
dropout增加了迭代次数。

---
### Details of learning
我们使用随机梯度下降训练我们的模型，批量大小为128个示例，动量为0.9，重量衰减为0.0005。 我们发现，这种少量的重量衰减对于模型学习很重要。换句话说，这里的重量衰减不仅仅是一个正则化器：它减少了
模型的训练误差。权重W的更新规则是:$$v_{i+1}\; :=\; 0.9v_{i}-0.0005\epsilon \omega _{i}-\epsilon \left( \frac{\partial L}{\partial w}\left| w_{i} \right| \right)_{D_{i}}$$$$\omega _{i+1}:=\omega _{i}+v_{i+1}$$
i是迭代次数，v是动量变量，$\epsilon $是学习率，$\left( \frac{\partial L}{\partial w}\left| w_{i} \right| \right)_{D_{i}}$是w导数的第i个批次Di导数的均值。

我们在每一层初始化权重从一个均值是0，标准差是0.01的标准高斯分布。我们初始化神经网络的bias在第2、4、5卷积神经网络和在全连接隐藏层通过常量1。该初始化通过向ReLU提供正输入来加速学习的早期阶段，
我们用常数0初始化剩余层中的神经元偏差。

我们对所有图层使用了相同的学习率，我们在整个培训过程中手动调整。 我们遵循的启发式是当验证错误率随着当前学习速率而停止改善时将学习速率除以10。 学习率初始化为0.01，并在终止前减少三次。 我们通过120万张图像的训练集训练网络大约90个周期，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。

---
### Result
