https://www.sohu.com/a/142920778_717210
https://www.sohu.com/a/114464910_465975 
https://www.cnblogs.com/iloveai/p/word2vec.html  

# 自然语言处理
---
- 
自然语言具有很高的抽象，相较于音频、图片z、视频等低层级的表示，自然语言的处理要难的多。如何有效的通过计算机语言表示自然语言将会很大影响机器学习、深度学习框架的模型h泛化能力。
我们从自然语言的表征出发来了解自然语言处理的进程。

## one-hot哑变量编码/独热向量、分布式表示  representation
---

### 1. one-hot哑变量编码
通过建立强大的词库，将每一个词表示为一个one-hot向量。such as [1,0,0,0,0,0,0,0,0,0],这种简单的表示方式配合最大熵、SVM、CRF等等算法可以很好地完成NLP领域的主流任务。编程上可以通过hash，分配给每一个词一个标记
缺点: 随着句子的长度变大，向量的维度也在变大，计算量将成几何增大。  任意两个词之间都是孤立的，无法在语义层面表示两个词之间的联系，这一点是致命的。

### 2.distributed representation
分布假说: 上下文相识的词，语义也相同。
分布式根据建模的不同分为: 基于矩阵的d分布表示，基于聚类的分布表示，基于神经网络的分布表示。
原理: 选择一种方式描述上下文，选择一种模型刻画目标词和上下文直接的联系

## 语言模型
---
- 文法语言模型
- 统计语言模型

### 1.统计语言模型
统计语言模型把语言(词的序列)看成是一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。给定一个集合V，对于V中的词构成的序列S<w1,w2,w3,w4,,,wn>,统计语言模型赋予这个序列一个概率P(s),来描述s符合集合V中语言语法和语义规则的置信度。 统计语言模型计算一个句子出现的概率，概率越大越符合集合中的语法和语义的规则。
常见的统计语言模型有N元文法模型 N-gram(unigram-model,bigram-model,trigram-model),统计语言的意义在于为一段句子确定一个概率分布p(w1,w2,w3,w4,w5,,,wm), 其中p(wi | w1,w1,,,wi-1)~p(wi| wi-(n-1),,wi-1)),通常x这些方法能保存一定的上下文信息。

### 基于矩阵的分布表示
分布语义模型，矩阵中的每一行就成为了对应词的表示，这种描述了上下文的分布。因此两个词的相似度可以转换为对应词的向量之间的差距。 Global-Vector

### 基于神经网络的语义模型
基于神经网络的表示一般称为词向量或者词嵌入word embedding，distributed representation。通过神经网络技术对上下文和目标词之间进行建模。由于神经网络比较灵活，可以表示复杂的上下文。基于矩阵的分布语义模型如果使用包含词序信息的n-gram作为上下文，随着n的增加，n-gramde数量会几何增长，带来维度灾难。n=3 3,2,1 。神经网络用n-gram作为上下文的时候，随着n的增加，神经网络是线性的组合，参数个数也是线性的增加。神经网络对于负复杂的模型就有了这一优势。词向量是神经网络训练语言模型的副产品。NNLM

### 理解
将one-hot的值从整数型改为浮点型，固定one-hot的维度得到word-vector， 将one-hot嵌入到有限维的空间。

### 神经网络模型
通过神经网络训练可以得到词向量
神经网络语言模型如下
- NNML
- LBL Log-Billinear Language Model
- RNNLM Recurrent Neural Network based Language Model
- C&W Collobert and Weston 在2008年学到的
- CBOW(continuous bag of word) 前面几个词或者后面几个词来计算某个词出现的概率 ， CBOW貌似快一点
- Skip-gram 根据某个词计算前后某几个连续的单词的概率

word2vector 是实现语言模型的工具m，C&W是SENNA


